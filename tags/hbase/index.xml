<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>hbase on FIFCOM实验室</title>
    <link>https://blog.fifcom.cn/tags/hbase/</link>
    <description>Recent content in hbase on FIFCOM实验室</description>
    <image>
      <url>https://fifcom.cn/avatar/?transparent=1</url>
      <link>https://fifcom.cn/avatar/?transparent=1</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 09 Apr 2022 00:00:00 +0800</lastBuildDate><atom:link href="https://blog.fifcom.cn/tags/hbase/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>HBase JavaAPI 数据操作</title>
      <link>https://blog.fifcom.cn/posts/hadoop-hbase-java-api-data-ops/</link>
      <pubDate>Sat, 09 Apr 2022 00:00:00 +0800</pubDate>
      
      <guid>https://blog.fifcom.cn/posts/hadoop-hbase-java-api-data-ops/</guid>
      <description>环境准备：在HBase shell中创建一个新表curd， 包含两个列族cfa和cfb:
create &#39;curd&#39;, &#39;cfa&#39;, &#39;cfb&#39;
为行键修改/添加列族和值 使用put操作进行添加或修改数据
package demo;  import java.io.IOException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.hbase.HBaseConfiguration; import org.apache.hadoop.hbase.TableName; import org.apache.hadoop.hbase.client.*; import org.apache.hadoop.hbase.util.Bytes;  class HBaseAPI { 	private final Connection conn; 	private final Admin admin;  	public HBaseAPI() throws IOException { 	Configuration conf = HBaseConfiguration.create(); 	conf.addResource(&amp;#34;hbase-site.xml&amp;#34;); 	conn = ConnectionFactory.createConnection(conf); 	admin = conn.getAdmin(); 	}  	public void put(String tb, String row, String[][] cols) throws IOException { 	// 获取表的实例 	Table table = conn.</description>
    </item>
    
    <item>
      <title>HBase JavaAPI 表操作</title>
      <link>https://blog.fifcom.cn/posts/hadoop-hbase-java-api-table-ops/</link>
      <pubDate>Thu, 07 Apr 2022 00:00:00 +0800</pubDate>
      
      <guid>https://blog.fifcom.cn/posts/hadoop-hbase-java-api-table-ops/</guid>
      <description>连接到HBase  创建配置对象并从本地载入配置文件 使用配置对象作为参数创建连接对象conn conn.getAdmin()返回管理HBase集群的Admin对象 此时已连接到HBase 可以调用Admin对象中的listTableNames()返回TableName[]，包含HBase中的所有表  package demo;  import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.hbase.HBaseConfiguration; import org.apache.hadoop.hbase.TableName; import org.apache.hadoop.hbase.client.Admin; import org.apache.hadoop.hbase.client.Connection; import org.apache.hadoop.hbase.client.ConnectionFactory;  import java.io.IOException;   public class Main { 	public static void main(String[] args) throws IOException { 	// load configuration and connect to HBase 	Configuration config = HBaseConfiguration.create(); 	config.addResource(&amp;#34;/hbase-site.xml&amp;#34;); 	Connection conn = ConnectionFactory.createConnection(config); 	Admin admin = conn.getAdmin(); 	// print all tables 	for (TableName tableName : admin.</description>
    </item>
    
    <item>
      <title>HBase JavaAPI 配置</title>
      <link>https://blog.fifcom.cn/posts/hadoop-hbase-java-api-config/</link>
      <pubDate>Tue, 05 Apr 2022 00:00:00 +0800</pubDate>
      
      <guid>https://blog.fifcom.cn/posts/hadoop-hbase-java-api-config/</guid>
      <description>这里演示在Windows上运行示例
配置Hosts 在Windows的Hosts文件夹中加入集群的所有节点
1.1.1.1 master2.2.2.2 slave1 配置Hadoop  复制已在集群上安装的Hadoop文件到本地。文件比较多，可以压缩后复制  从master上复制
# 压缩文件夹 cd /opt/ zip -r hadoop.zip /hadoop # 将/opt/hadoop.zip复制到本地  解压hadoop.zip到本地某目录，注意：目录中不能有空格
  编辑hadoop\etc\hadoop\hadoop-env.cmd中的JAVA_HOME
  :: 首先在cmd里找到JAVA_HOME echo %JAVA_HOME% :: =&amp;gt; D:\Program_Files\Semeru\jdk-17.0.1.12-openj9\ :: 编辑hadoop-env.cmd中的JAVA_HOME :: =&amp;gt; JAVA_HOME=D:\Program_Files\Semeru\jdk-17.0.1.12-openj9\ 配置HADOOP环境变量  在系统变量中加入
HADOOP_HOME=hadoop_path_hereHADOOP_USER_NAME=root 在path中加入
%HADOOP_HOME%\bin%HADOOP_HOME%\sbin  将winutils复制进hadoop\bin中
  测试hadoop version，正常则成功   创建测试数据 在hbase shell中:
create &amp;#39;scores&amp;#39;,&amp;#39;grade&amp;#39;,&amp;#39;course&amp;#39; put &amp;#39;scores&amp;#39;,&amp;#39;1001&amp;#39;,&amp;#39;grade:&amp;#39;,&amp;#39;1&amp;#39; put &amp;#39;scores&amp;#39;,&amp;#39;1001&amp;#39;,&amp;#39;course:art&amp;#39;,&amp;#39;80&amp;#39; put &amp;#39;scores&amp;#39;,&amp;#39;1001&amp;#39;,&amp;#39;course:math&amp;#39;,&amp;#39;89&amp;#39; put &amp;#39;scores&amp;#39;,&amp;#39;1002&amp;#39;,&amp;#39;grade:&amp;#39;,&amp;#39;2&amp;#39; put &amp;#39;scores&amp;#39;,&amp;#39;1002&amp;#39;,&amp;#39;course:art&amp;#39;,&amp;#39;87&amp;#39; put &amp;#39;scores&amp;#39;,&amp;#39;1002&amp;#39;,&amp;#39;course:math&amp;#39;,&amp;#39;57&amp;#39; put &amp;#39;scores&amp;#39;,&amp;#39;1003&amp;#39;,&amp;#39;course:math&amp;#39;,&amp;#39;90&amp;#39; # 查看插入的数据 scan &amp;#34;scores&amp;#34; 创建工程 这里用到的IDE是eclipse</description>
    </item>
    
    <item>
      <title>HBase常用命令</title>
      <link>https://blog.fifcom.cn/posts/hadoop-hbase-commands/</link>
      <pubDate>Mon, 04 Apr 2022 00:00:00 +0800</pubDate>
      
      <guid>https://blog.fifcom.cn/posts/hadoop-hbase-commands/</guid>
      <description>环境准备 安装并启动HBase
这里主要记录HBase shell的命令。进入HBase shell:
hbase shell 帮助文档 使用help查看所有命令
help 可以使用help &amp;quot;create&amp;quot;来查看具体命令的帮助
命名空间 namespace指对一组表的逻辑分组，类似DBMS中的一个数据库，方便对表在业务上划分。
常用操作：
 创建命名空间：create_namespace &amp;quot;test&amp;quot; 列出所有命名空间：list_namespace   可以看到3个namespace，其中hbase是系统内建的；用户建表时未指定命名空间的表都放在default里。 3. 查看描述：describe_namespace &amp;quot;test&amp;quot; 4. 列出命名空间下的所有表：list_namespace_tables &amp;quot;hbase&amp;quot; 可以看到内建的命名空间hbase中有两个表：meta和namespace
建表  不指定命名空间建表：  # create &amp;#39;表名&amp;#39;, &amp;#39;列名a&amp;#39;, &amp;#39;列名b&amp;#39; create &amp;#39;tb_a&amp;#39;, &amp;#39;col_a&amp;#39;, &amp;#39;col_b&amp;#39; 即在default命名空间下创建了表tb_a，包含列名col_a和col_b 指定命名空间建表：  # create &amp;#39;命名空间:表名&amp;#39;, &amp;#39;列名a&amp;#39;, &amp;#39;列名b&amp;#39; create &amp;#39;test:tb_b&amp;#39;, &amp;#39;col_a&amp;#39;, &amp;#39;col_b&amp;#39; 即在test命名空间下创建了表tb_b，包含列名col_a和col_b 查看所有表：  list # [&amp;#34;tb_a&amp;#34;, &amp;#34;test:tb_b&amp;#34;] 判断表是否存在：  exists &amp;#34;tb_a&amp;#34; # =&amp;gt; true exists &amp;#34;tb_c&amp;#34; # =&amp;gt; false 插入数据 使用put命令插入数据</description>
    </item>
    
    <item>
      <title>HBase安装</title>
      <link>https://blog.fifcom.cn/posts/hadoop-hbase-install/</link>
      <pubDate>Wed, 30 Mar 2022 00:00:00 +0800</pubDate>
      
      <guid>https://blog.fifcom.cn/posts/hadoop-hbase-install/</guid>
      <description>安装HBase 环境准备 首先应完成完全分布式安装Hadoop
并配置免密登录、上传Zookeeper、HBase安装包至master节点
配置Zookeeper 解压Zookeeper安装包
tar -zxvf apache-zookeeper-3.8.0-bin.tar.gz -C /opt 在解压目录创建data和log目录
cd /opt/apache-zookeeper-3.8.0-bin mkdir data log 编辑配置文件：复制conf目录下的样例文件，并修改dataDir(数据文件目录)和dataLogDir(日志文件目录)
cd conf cp zoo_sample.cfg zoo.cfg vi zoo.cfg =&amp;gt; dataDir=/opt/apache-zookeeper-3.8.0-bin/data dataLogDir=/opt/apache-zookeeper-3.8.0-bin/log 在data目录中新建myid文件，向里面写入1。这里的1表示是集群的第几台机器，slave节点则这里为2，3，4&amp;hellip;
cd ../data/ echo 1 &amp;gt;&amp;gt; myid 再次编辑zoo.cfg，添加主节点和从节点地址
cd ../conf/ vi zoo.cfg =&amp;gt; # 底部添加 server.1=master:2888:3888 server.2=slave1:2888:3888 server.3=slave2:2888:3888 接着将Zookeeper安装文件分发到子节点
scp -r /opt/apache-zookeeper-3.8.0-bin/ slave1:/opt/ scp -r /opt/apache-zookeeper-3.8.0-bin/ slave2:/opt/ 修改子节点上的myid
cd /opt/apache-zookeeper-3.8.0-bin/data/ echo 2 &amp;gt; myid # echo 3 &amp;gt; myid cat myid 启动Zookeeper 启动前需关闭防火墙，以及在master上启动Hadoop</description>
    </item>
    
  </channel>
</rss>
