<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>hadoop | FIFCOM实验室</title><meta name=keywords content>
<meta name=description content="咕→咕↑咕↓↑咕↓">
<meta name=author content="FIFCOM">
<link rel=canonical href=https://blog.fifcom.cn/tags/hadoop/>
<meta name=google-site-verification content="XYZabc">
<meta name=yandex-verification content="XYZabc">
<meta name=msvalidate.01 content="XYZabc">
<link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<link rel=preload href="https://fifcom.cn/avatar/?transparent=1" as=image>
<link rel=icon href="https://fifcom.cn/avatar/?transparent=1">
<link rel=icon type=image/png sizes=16x16 href="https://fifcom.cn/avatar/?transparent=1">
<link rel=icon type=image/png sizes=32x32 href="https://fifcom.cn/avatar/?transparent=1">
<link rel=apple-touch-icon href="https://fifcom.cn/avatar/?transparent=1">
<link rel=mask-icon href="https://fifcom.cn/avatar/?transparent=1">
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.93.2">
<link rel=alternate type=application/rss+xml href=https://blog.fifcom.cn/tags/hadoop/index.xml>
<noscript>
<style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="hadoop">
<meta property="og:description" content="咕→咕↑咕↓↑咕↓">
<meta property="og:type" content="website">
<meta property="og:url" content="https://blog.fifcom.cn/tags/hadoop/"><meta property="og:image" content="https://fifcom.cn/avatar/?transparent=1"><meta property="og:site_name" content="FIFCOM实验室">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://fifcom.cn/avatar/?transparent=1">
<meta name=twitter:title content="hadoop">
<meta name=twitter:description content="咕→咕↑咕↓↑咕↓">
</head><body class=list id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://blog.fifcom.cn/ accesskey=h title="FIFCOM实验室 (Alt + H)">
<img src="https://fifcom.cn/avatar/?transparent=1" alt=logo aria-label=logo height=35>FIFCOM实验室</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div><ul id=menu>
<li>
<a href=https://blog.fifcom.cn/tags/ title=标签>
<span>标签</span>
</a>
</li><li>
<a href=https://fifcom.cn/ title=主页>
<span>主页</span>
</a>
</li><li>
<a href=https://blog.fifcom.cn/tags/friends/ title=朋友们>
<span>朋友们</span>
</a>
</li></ul></nav></header><main class=main>
<header class=page-header><div class=breadcrumbs><a href=https://blog.fifcom.cn/>Home</a>&nbsp;»&nbsp;<a href=https://blog.fifcom.cn/tags/>Tags</a></div><h1>hadoop</h1></header><article class="post-entry tag-entry">
<header class=entry-header>
<h2>HBase安装
</h2></header><section class=entry-content>
<p>安装HBase 环境准备 首先应完成完全分布式安装Hadoop
并配置免密登录、上传Zookeeper、HBase安装包至master节点
配置Zookeeper 解压Zookeeper安装包
tar -zxvf apache-zookeeper-3.8.0-bin.tar.gz -C /opt 在解压目录创建data和log目录
cd /opt/apache-zookeeper-3.8.0-bin mkdir data log 编辑配置文件：复制conf目录下的样例文件，并修改dataDir(数据文件目录)和dataLogDir(日志文件目录)
cd conf cp zoo_sample.cfg zoo.cfg vi zoo.cfg => dataDir=/opt/apache-zookeeper-3.8.0-bin/data dataLogDir=/opt/apache-zookeeper-3.8.0-bin/log 在data目录中新建myid文件，向里面写入1。这里的1表示是集群的第几台机器，slave节点则这里为2，3，4…
cd ../data/ echo 1 >> myid 再次编辑zoo.cfg，添加主节点和从节点地址
cd ../conf/ vi zoo.cfg => # 底部添加 server.1=master:2888:3888 server.2=slave1:2888:3888 server.3=slave2:2888:3888 接着将Zookeeper安装文件分发到子节点
scp -r /opt/apache-zookeeper-3.8.0-bin/ slave1:/opt/ scp -r /opt/apache-zookeeper-3.8.0-bin/ slave2:/opt/ 修改子节点上的myid
cd /opt/apache-zookeeper-3.8.0-bin/data/ echo 2 > myid # echo 3 > myid cat myid 启动Zookeeper 启动前需关闭防火墙，以及在master上启动Hadoop...</p></section><footer class=entry-footer><span title="2022-03-30 00:00:00 +0800 CST">March 30, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;FIFCOM</footer><a class=entry-link aria-label="post link to HBase安装" href=https://blog.fifcom.cn/posts/hadoop-hbase-install/></a>
</article><article class="post-entry tag-entry">
<header class=entry-header>
<h2>Hadoop 运行官方示例WordCount
</h2></header><section class=entry-content>
<p>运行官方示例WordCount 环境安装完成后输入jps确保有5+个进程： 本实验使用官方WordCount程序实现Java源码词语统计。
准备Java源码 首先进入Java安装目录，找到src.zip，解压到tmp中
cd /opt/jdk8u282-b08/ unzip -d tmp src.zip 接着将tmp中的所有.java文件拷贝到javasrc目录下，需要先创建javasrc文件夹
mkdir javasrc # 找到文件名匹配*.java的文件后执行 cp 命令复制文件。注意\;是-exec的结束标志 find tmp -name *.java -exec cp {} javasrc \; 上传文件至HDFS 由于Java源码大多为小文件，需要将这些文件合并成一个大文件。如果不这样做的话会由于io开销过大而导致运行时间过长。
此处使用appendToFile将本地的所有java文件上传到HDFS的input/data_file中
cd /opt/jdk8u282-b08 # 文件会上传到HDFS的/user/&lt;login_user>/input/data_file中， # 该目录如果不存在则会自动创建 hadoop fs -appendToFile javasrc/*.java input/data_file hadoop fs -ls input hadoop fs -tail input/data_file # 查看末尾1KB的内容 运行WordCount 官方示例文件在/opt/hadoop~/share/hadoop/mapreduce/hadoop-mapreduce-examples-~.jar中
hadoop jar命令用于执行jar文件，wordcount是例子的主类[?]，input是输入文件所在目录，output是输出文件所在目录。output目录不能事先存在。
hadoop jar /opt/hadoop-2.9.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar wordcount input output 可以看到map和reduce操作是并行进行的
查看输出结果 查看HDFS中的output文件夹
hadoop fs -ls output 当运行成功后会在该文件下生成一个_SUCCESS文件，该文件长度为0，仅预示着成功输出了结果，其实际结果 存放在part-r-xxxxx文件中...</p></section><footer class=entry-footer><span title="2022-03-15 00:00:00 +0800 CST">March 15, 2022</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;FIFCOM</footer><a class=entry-link aria-label="post link to Hadoop 运行官方示例WordCount" href=https://blog.fifcom.cn/posts/hadoop-wordcount-example/></a>
</article><article class="post-entry tag-entry">
<header class=entry-header>
<h2>Hadoop 常用命令
</h2></header><section class=entry-content>
<p>Hadoop常用命令 版本号 hadoop version 创建文件夹 hdfs dfs -mkdir /dir1 # 根目录下创建 列出文件和文件夹 展示根目录下所有文件和文件夹，及其信息
hdfs dfs -ls / 递归展示子文件夹中的内容
hdfs dfs -ls -R / 复制本地文件至HDFS put或copyFromLocal命令, 使用-f强制覆盖HDFS中已存在的文件
touch aaa.txt hdfs dfs -put aaa.txt /dir1 # hdfs dfs -copyFromLocal aaa.txt /dir1 hdfs dfs -ls -R / 获取HDFS中的文件 get / copyToLocal: 将单个文件复制至本地 # rm aaa.txt hdfs dfs -get /dir1/aaa.txt ~ # hdfs dfs -copyToLocal /dir1/aaa.txt ~ ls -al getmerge: 将多个文件存入本地的同一个文件中 touch a....</p></section><footer class=entry-footer><span title="2022-03-10 00:00:00 +0800 CST">March 10, 2022</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;FIFCOM</footer><a class=entry-link aria-label="post link to Hadoop 常用命令" href=https://blog.fifcom.cn/posts/hadoop-commands/></a>
</article><article class="post-entry tag-entry">
<header class=entry-header>
<h2>Hadoop 完全分布式安装
</h2></header><section class=entry-content>
<p>基本环境准备 配置Java环境 解压JDK安装包
tar -zxvf OpenJDK8U-jdk_x64_linux_openj9_linuxXL_8u282b08_openj9-0.24.0.tar.gz -C /opt 配置环境变量
vi /etc/profile # 在文件下方添加 export JAVA_HOME=/opt/jdk8u282-b08 export JRE_HOME=/opt/jdk8u282-b08/jre export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar # 输入:wq!退出 source /etc/profile # 使用环境变量配置 配置hosts 所有机器都要配置，以便设置免密登录
编辑/etc/hosts:
master_ip_here master slave1_ip_here slave1 slave2_ip_here slave2 配置免密登录 在所有节点上生成rsa公钥和私钥: ssh-keygen -t rsa 将slave节点上生成的公钥用scp命令传到master节点上： scp ~/.ssh/id_rsa.pub root@master:~/.ssh/id_rsa.pub.slave1 scp ~/.ssh/id_rsa.pub root@master:~/.ssh/id_rsa.pub.slave2 3. 在master上生成authorized_keys，即把生成的公钥输入进一个文件：
cd ~/.ssh cat id_rsa.pub >> authorized_keys cat id_rsa.pub.slave1 >> authorized_keys cat id_rsa.pub.slave2 >> authorized_keys 将master上的authorized_keys用scp命令传到slave节点上： scp ~/....</p></section><footer class=entry-footer><span title="2022-03-05 00:00:00 +0800 CST">March 5, 2022</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;FIFCOM</footer><a class=entry-link aria-label="post link to Hadoop 完全分布式安装" href=https://blog.fifcom.cn/posts/hadoop-distributed-install/></a>
</article><article class="post-entry tag-entry">
<header class=entry-header>
<h2>Hadoop 伪分布式安装
</h2></header><section class=entry-content>
<p>环境配置 三台机器上都安装JDK环境: 上传JDK安装包，解压后配置环境变量即可 tar -zxvf OpenJDK8U-jdk_x64_linux_openj9_linuxXL_8u282b08_openj9-0.24.0.tar.gz -C /opt vi /etc/profile # 在文件下方添加 export JAVA_HOME=/opt/jdk8u282-b08 export JRE_HOME=/opt/jdk8u282-b08/jre export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar # 输入:wq!退出 source /etc/profile # 使用环境变量配置 安装Hadoop环境: 首先上传安装包，再解压至opt目录 tar -zxvf hadoop-2.9.2.tar.gz -C /opt hadoop前提配置 配置master节点 首先进入安装目录, 编辑脚本:
cd /opt/hadoop-2.9.2/etc/hadoop vi hadoop-env.sh # 找到`# The java implementation to use. ...`， 编辑下面的为 export JAVA_HOME=/opt/jdk8u282-b08 继续编辑当前目录下的core-site.xml文件，指定HDFS的 NameNode的地址，value值是主机名加端口号，主机使用master节点的ip地址
vi core-site.xml 在configuration标签内添加：
&lt;property> &lt;name>fs.default.name&lt;/name> &lt;value>hdfs://master_ip_here:9000&lt;/value> &lt;/property> &lt;property> &lt;name>fs....</p></section><footer class=entry-footer><span title="2022-03-01 00:00:00 +0800 CST">March 1, 2022</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;FIFCOM</footer><a class=entry-link aria-label="post link to Hadoop 伪分布式安装" href=https://blog.fifcom.cn/posts/hadoop-pseudo-distributed-install/></a>
</article></main><footer class=footer>
<span>&copy; 2022 <a href=https://blog.fifcom.cn/>FIFCOM实验室</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script>
</body></html>