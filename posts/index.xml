<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on FIFCOM实验室</title>
    <link>https://blog.fifcom.cn/posts/</link>
    <description>Recent content in Posts on FIFCOM实验室</description>
    <image>
      <url>https://fifcom.cn/avatar/?transparent=1</url>
      <link>https://fifcom.cn/avatar/?transparent=1</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 11 Apr 2022 00:00:00 +0800</lastBuildDate><atom:link href="https://blog.fifcom.cn/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Phoenix操作HBase</title>
      <link>https://blog.fifcom.cn/posts/hbase-phoenix/</link>
      <pubDate>Mon, 11 Apr 2022 00:00:00 +0800</pubDate>
      
      <guid>https://blog.fifcom.cn/posts/hbase-phoenix/</guid>
      <description>Phoenix可以使用标准SQL来操作HBase
安装前建议关闭HBase、Zookeeper、Hadoop服务
下载Phoenix 首先应下载Phoenix: Apache Phoenix ，下载apache-phoenix-5.0.0-HBase-2.0-bin.tar.gz，并将其上传至master节点
安装Phoenix 解压到opt目录
tar -zxvf apache-phoenix-5.0.0-HBase-2.0-bin.tar.gz -C /opt/ cd /opt mv apache-phoenix-5.0.0-HBase-2.0-bin/ phoenix/ 复制phoenix目录下的两个jar文件到hbase/lib下
cd /opt/phoenix cp phoenix-5.0.0-HBase-2.0-client.jar /opt/hbase/lib/ cp phoenix-core-5.0.0-HBase-2.0.jar /opt/hbase/lib/ 并将其分发到所有子节点上
scp phoenix-5.0.0-HBase-2.0-client.jar slave1:/opt/hbase/lib/ scp phoenix-core-5.0.0-HBase-2.0.jar slave1:/opt/hbase/lib/ 配置Phoenix 将Hadoop的和HBase的配置文件复制到Phoenix/bin下
cp /opt/hadoop/etc/hadoop/core-site.xml /opt/phoenix/bin/ cp /opt/hadoop/etc/hadoop/hdfs-site.xml /opt/phoenix/bin/ cp /opt/hbase/conf/hbase-site.xml /opt/phoenix/bin/ 在master上配置环境变量
vi /etc/profile =&amp;gt; # 最后添加 export PHOENIX_HOME=/opt/phoenix export PHOENIX_CLASSPATH=$PHOENIX_HOME export PATH=$PATH:$PHOENIX_HOME/bin &amp;lt;= # :wq source /etc/profile 设置脚本为可执行
cd /opt/phoenix/bin chmod +x psql.py chmod +x sqlline.</description>
    </item>
    
    <item>
      <title>HBase JavaAPI 数据操作</title>
      <link>https://blog.fifcom.cn/posts/hadoop-hbase-java-api-data-ops/</link>
      <pubDate>Sat, 09 Apr 2022 00:00:00 +0800</pubDate>
      
      <guid>https://blog.fifcom.cn/posts/hadoop-hbase-java-api-data-ops/</guid>
      <description>环境准备：在HBase shell中创建一个新表curd， 包含两个列族cfa和cfb:
create &#39;curd&#39;, &#39;cfa&#39;, &#39;cfb&#39;
为行键修改/添加列族和值 使用put操作进行添加或修改数据
package demo;  import java.io.IOException; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.hbase.HBaseConfiguration; import org.apache.hadoop.hbase.TableName; import org.apache.hadoop.hbase.client.*; import org.apache.hadoop.hbase.util.Bytes;  class HBaseAPI { 	private final Connection conn; 	private final Admin admin;  	public HBaseAPI() throws IOException { 	Configuration conf = HBaseConfiguration.create(); 	conf.addResource(&amp;#34;hbase-site.xml&amp;#34;); 	conn = ConnectionFactory.createConnection(conf); 	admin = conn.getAdmin(); 	}  	public void put(String tb, String row, String[][] cols) throws IOException { 	// 获取表的实例 	Table table = conn.</description>
    </item>
    
    <item>
      <title>HBase JavaAPI 表操作</title>
      <link>https://blog.fifcom.cn/posts/hadoop-hbase-java-api-table-ops/</link>
      <pubDate>Thu, 07 Apr 2022 00:00:00 +0800</pubDate>
      
      <guid>https://blog.fifcom.cn/posts/hadoop-hbase-java-api-table-ops/</guid>
      <description>连接到HBase  创建配置对象并从本地载入配置文件 使用配置对象作为参数创建连接对象conn conn.getAdmin()返回管理HBase集群的Admin对象 此时已连接到HBase 可以调用Admin对象中的listTableNames()返回TableName[]，包含HBase中的所有表  package demo;  import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.hbase.HBaseConfiguration; import org.apache.hadoop.hbase.TableName; import org.apache.hadoop.hbase.client.Admin; import org.apache.hadoop.hbase.client.Connection; import org.apache.hadoop.hbase.client.ConnectionFactory;  import java.io.IOException;   public class Main { 	public static void main(String[] args) throws IOException { 	// load configuration and connect to HBase 	Configuration config = HBaseConfiguration.create(); 	config.addResource(&amp;#34;/hbase-site.xml&amp;#34;); 	Connection conn = ConnectionFactory.createConnection(config); 	Admin admin = conn.getAdmin(); 	// print all tables 	for (TableName tableName : admin.</description>
    </item>
    
    <item>
      <title>HBase JavaAPI 配置</title>
      <link>https://blog.fifcom.cn/posts/hadoop-hbase-java-api-config/</link>
      <pubDate>Tue, 05 Apr 2022 00:00:00 +0800</pubDate>
      
      <guid>https://blog.fifcom.cn/posts/hadoop-hbase-java-api-config/</guid>
      <description>这里演示在Windows上运行示例
配置Hosts 在Windows的Hosts文件夹中加入集群的所有节点
1.1.1.1 master2.2.2.2 slave1 配置Hadoop  复制已在集群上安装的Hadoop文件到本地。文件比较多，可以压缩后复制  从master上复制
# 压缩文件夹 cd /opt/ zip -r hadoop.zip /hadoop # 将/opt/hadoop.zip复制到本地  解压hadoop.zip到本地某目录，注意：目录中不能有空格
  编辑hadoop\etc\hadoop\hadoop-env.cmd中的JAVA_HOME
  :: 首先在cmd里找到JAVA_HOME echo %JAVA_HOME% :: =&amp;gt; D:\Program_Files\Semeru\jdk-17.0.1.12-openj9\ :: 编辑hadoop-env.cmd中的JAVA_HOME :: =&amp;gt; JAVA_HOME=D:\Program_Files\Semeru\jdk-17.0.1.12-openj9\ 配置HADOOP环境变量  在系统变量中加入
HADOOP_HOME=hadoop_path_hereHADOOP_USER_NAME=root 在path中加入
%HADOOP_HOME%\bin%HADOOP_HOME%\sbin  将winutils复制进hadoop\bin中
  测试hadoop version，正常则成功   创建测试数据 在hbase shell中:
create &amp;#39;scores&amp;#39;,&amp;#39;grade&amp;#39;,&amp;#39;course&amp;#39; put &amp;#39;scores&amp;#39;,&amp;#39;1001&amp;#39;,&amp;#39;grade:&amp;#39;,&amp;#39;1&amp;#39; put &amp;#39;scores&amp;#39;,&amp;#39;1001&amp;#39;,&amp;#39;course:art&amp;#39;,&amp;#39;80&amp;#39; put &amp;#39;scores&amp;#39;,&amp;#39;1001&amp;#39;,&amp;#39;course:math&amp;#39;,&amp;#39;89&amp;#39; put &amp;#39;scores&amp;#39;,&amp;#39;1002&amp;#39;,&amp;#39;grade:&amp;#39;,&amp;#39;2&amp;#39; put &amp;#39;scores&amp;#39;,&amp;#39;1002&amp;#39;,&amp;#39;course:art&amp;#39;,&amp;#39;87&amp;#39; put &amp;#39;scores&amp;#39;,&amp;#39;1002&amp;#39;,&amp;#39;course:math&amp;#39;,&amp;#39;57&amp;#39; put &amp;#39;scores&amp;#39;,&amp;#39;1003&amp;#39;,&amp;#39;course:math&amp;#39;,&amp;#39;90&amp;#39; # 查看插入的数据 scan &amp;#34;scores&amp;#34; 创建工程 这里用到的IDE是eclipse</description>
    </item>
    
    <item>
      <title>HBase常用命令</title>
      <link>https://blog.fifcom.cn/posts/hadoop-hbase-commands/</link>
      <pubDate>Mon, 04 Apr 2022 00:00:00 +0800</pubDate>
      
      <guid>https://blog.fifcom.cn/posts/hadoop-hbase-commands/</guid>
      <description>环境准备 安装并启动HBase
这里主要记录HBase shell的命令。进入HBase shell:
hbase shell 帮助文档 使用help查看所有命令
help 可以使用help &amp;quot;create&amp;quot;来查看具体命令的帮助
命名空间 namespace指对一组表的逻辑分组，类似DBMS中的一个数据库，方便对表在业务上划分。
常用操作：
 创建命名空间：create_namespace &amp;quot;test&amp;quot; 列出所有命名空间：list_namespace   可以看到3个namespace，其中hbase是系统内建的；用户建表时未指定命名空间的表都放在default里。 3. 查看描述：describe_namespace &amp;quot;test&amp;quot; 4. 列出命名空间下的所有表：list_namespace_tables &amp;quot;hbase&amp;quot; 可以看到内建的命名空间hbase中有两个表：meta和namespace
建表  不指定命名空间建表：  # create &amp;#39;表名&amp;#39;, &amp;#39;列名a&amp;#39;, &amp;#39;列名b&amp;#39; create &amp;#39;tb_a&amp;#39;, &amp;#39;col_a&amp;#39;, &amp;#39;col_b&amp;#39; 即在default命名空间下创建了表tb_a，包含列名col_a和col_b 指定命名空间建表：  # create &amp;#39;命名空间:表名&amp;#39;, &amp;#39;列名a&amp;#39;, &amp;#39;列名b&amp;#39; create &amp;#39;test:tb_b&amp;#39;, &amp;#39;col_a&amp;#39;, &amp;#39;col_b&amp;#39; 即在test命名空间下创建了表tb_b，包含列名col_a和col_b 查看所有表：  list # [&amp;#34;tb_a&amp;#34;, &amp;#34;test:tb_b&amp;#34;] 判断表是否存在：  exists &amp;#34;tb_a&amp;#34; # =&amp;gt; true exists &amp;#34;tb_c&amp;#34; # =&amp;gt; false 插入数据 使用put命令插入数据</description>
    </item>
    
    <item>
      <title>HBase安装</title>
      <link>https://blog.fifcom.cn/posts/hadoop-hbase-install/</link>
      <pubDate>Wed, 30 Mar 2022 00:00:00 +0800</pubDate>
      
      <guid>https://blog.fifcom.cn/posts/hadoop-hbase-install/</guid>
      <description>安装HBase 环境准备 首先应完成完全分布式安装Hadoop
并配置免密登录、上传Zookeeper、HBase安装包至master节点
配置Zookeeper 解压Zookeeper安装包
tar -zxvf apache-zookeeper-3.8.0-bin.tar.gz -C /opt 在解压目录创建data和log目录
cd /opt/apache-zookeeper-3.8.0-bin mkdir data log 编辑配置文件：复制conf目录下的样例文件，并修改dataDir(数据文件目录)和dataLogDir(日志文件目录)
cd conf cp zoo_sample.cfg zoo.cfg vi zoo.cfg =&amp;gt; dataDir=/opt/apache-zookeeper-3.8.0-bin/data dataLogDir=/opt/apache-zookeeper-3.8.0-bin/log 在data目录中新建myid文件，向里面写入1。这里的1表示是集群的第几台机器，slave节点则这里为2，3，4&amp;hellip;
cd ../data/ echo 1 &amp;gt;&amp;gt; myid 再次编辑zoo.cfg，添加主节点和从节点地址
cd /opt/apache-zookeeper-3.8.0-bin/conf/ vi zoo.cfg =&amp;gt; # 底部添加 server.1=master:2888:3888 server.2=slave1:2888:3888 接着将Zookeeper安装文件分发到子节点
scp -r /opt/apache-zookeeper-3.8.0-bin/ slave1:/opt/ 修改子节点上的myid
cd /opt/apache-zookeeper-3.8.0-bin/data/ echo 2 &amp;gt; myid # echo 3 &amp;gt; myid cat myid 启动Zookeeper 启动前需关闭防火墙，以及在master上启动Hadoop
systemctl stop firewalld.service systemctl disable firewalld.</description>
    </item>
    
    <item>
      <title>Hadoop 运行官方示例WordCount</title>
      <link>https://blog.fifcom.cn/posts/hadoop-wordcount-example/</link>
      <pubDate>Tue, 15 Mar 2022 00:00:00 +0800</pubDate>
      
      <guid>https://blog.fifcom.cn/posts/hadoop-wordcount-example/</guid>
      <description>运行官方示例WordCount 环境安装完成后输入jps确保有5+个进程： 本实验使用官方WordCount程序实现Java源码词语统计。
准备Java源码 首先进入Java安装目录，找到src.zip，解压到tmp中
cd /opt/jdk8u282-b08/ unzip -d tmp src.zip 接着将tmp中的所有.java文件拷贝到javasrc目录下，需要先创建javasrc文件夹
mkdir javasrc # 找到文件名匹配*.java的文件后执行 cp 命令复制文件。注意\;是-exec的结束标志 find tmp -name *.java -exec cp {} javasrc \; 上传文件至HDFS 由于Java源码大多为小文件，需要将这些文件合并成一个大文件。如果不这样做的话会由于io开销过大而导致运行时间过长。
此处使用appendToFile将本地的所有java文件上传到HDFS的input/data_file中
cd /opt/jdk8u282-b08 # 文件会上传到HDFS的/user/&amp;lt;login_user&amp;gt;/input/data_file中， # 该目录如果不存在则会自动创建 hadoop fs -appendToFile javasrc/*.java input/data_file hadoop fs -ls input hadoop fs -tail input/data_file # 查看末尾1KB的内容 运行WordCount 官方示例文件在/opt/hadoop~/share/hadoop/mapreduce/hadoop-mapreduce-examples-~.jar中
hadoop jar命令用于执行jar文件，wordcount是例子的主类[?]，input是输入文件所在目录，output是输出文件所在目录。output目录不能事先存在。
hadoop jar /opt/hadoop-2.9.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.2.jar wordcount input output 可以看到map和reduce操作是并行进行的
查看输出结果 查看HDFS中的output文件夹
hadoop fs -ls output 当运行成功后会在该文件下生成一个_SUCCESS文件，该文件长度为0，仅预示着成功输出了结果，其实际结果 存放在part-r-xxxxx文件中</description>
    </item>
    
    <item>
      <title>Hadoop 常用命令</title>
      <link>https://blog.fifcom.cn/posts/hadoop-commands/</link>
      <pubDate>Thu, 10 Mar 2022 00:00:00 +0800</pubDate>
      
      <guid>https://blog.fifcom.cn/posts/hadoop-commands/</guid>
      <description>Hadoop常用命令 版本号 hadoop version 创建文件夹 hdfs dfs -mkdir /dir1 # 根目录下创建 列出文件和文件夹 展示根目录下所有文件和文件夹，及其信息
hdfs dfs -ls / 递归展示子文件夹中的内容
hdfs dfs -ls -R / 复制本地文件至HDFS put或copyFromLocal命令, 使用-f强制覆盖HDFS中已存在的文件
touch aaa.txt hdfs dfs -put aaa.txt /dir1 # hdfs dfs -copyFromLocal aaa.txt /dir1 hdfs dfs -ls -R / 获取HDFS中的文件  get / copyToLocal: 将单个文件复制至本地  # rm aaa.txt hdfs dfs -get /dir1/aaa.txt ~ # hdfs dfs -copyToLocal /dir1/aaa.txt ~ ls -al getmerge: 将多个文件存入本地的同一个文件中  touch a.</description>
    </item>
    
    <item>
      <title>Hadoop 完全分布式安装</title>
      <link>https://blog.fifcom.cn/posts/hadoop-distributed-install/</link>
      <pubDate>Sat, 05 Mar 2022 00:00:00 +0800</pubDate>
      
      <guid>https://blog.fifcom.cn/posts/hadoop-distributed-install/</guid>
      <description>基本环境准备 配置Java环境 解压JDK安装包
tar -zxvf OpenJDK8U-jdk_x64_linux_openj9_linuxXL_8u282b08_openj9-0.24.0.tar.gz -C /opt 配置环境变量
vi /etc/profile # 在文件下方添加 export JAVA_HOME=/opt/jdk8u282-b08 export JRE_HOME=/opt/jdk8u282-b08/jre export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar # 输入:wq!退出 source /etc/profile # 使用环境变量配置 配置hosts 所有机器都要配置，以便设置免密登录
编辑/etc/hosts:
master_ip_here master slave1_ip_here slave1 slave2_ip_here slave2 配置免密登录  在所有节点上生成rsa公钥和私钥: ssh-keygen -t rsa  将slave节点上生成的公钥用scp命令传到master节点上：  scp ~/.ssh/id_rsa.pub root@master:~/.ssh/id_rsa.pub.slave1 scp ~/.ssh/id_rsa.pub root@master:~/.ssh/id_rsa.pub.slave2 3. 在master上生成authorized_keys，即把生成的公钥输入进一个文件：
cd ~/.ssh cat id_rsa.pub &amp;gt;&amp;gt; authorized_keys cat id_rsa.pub.slave1 &amp;gt;&amp;gt; authorized_keys cat id_rsa.pub.slave2 &amp;gt;&amp;gt; authorized_keys 将master上的authorized_keys用scp命令传到slave节点上：  scp ~/.</description>
    </item>
    
    <item>
      <title>Hadoop 伪分布式安装</title>
      <link>https://blog.fifcom.cn/posts/hadoop-pseudo-distributed-install/</link>
      <pubDate>Tue, 01 Mar 2022 00:00:00 +0800</pubDate>
      
      <guid>https://blog.fifcom.cn/posts/hadoop-pseudo-distributed-install/</guid>
      <description>环境配置  三台机器上都安装JDK环境: 上传JDK安装包，解压后配置环境变量即可  tar -zxvf OpenJDK8U-jdk_x64_linux_openj9_linuxXL_8u282b08_openj9-0.24.0.tar.gz -C /opt vi /etc/profile # 在文件下方添加 export JAVA_HOME=/opt/jdk8u282-b08 export JRE_HOME=/opt/jdk8u282-b08/jre export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar # 输入:wq!退出 source /etc/profile # 使用环境变量配置 安装Hadoop环境: 首先上传安装包，再解压至opt目录  tar -zxvf hadoop-2.9.2.tar.gz -C /opt hadoop前提配置  配置master节点  首先进入安装目录, 编辑脚本:
cd /opt/hadoop-2.9.2/etc/hadoop vi hadoop-env.sh # 找到`# The java implementation to use. ...`， 编辑下面的为 export JAVA_HOME=/opt/jdk8u282-b08 继续编辑当前目录下的core-site.xml文件，指定HDFS的 NameNode的地址，value值是主机名加端口号，主机使用master节点的ip地址
vi core-site.xml 在configuration标签内添加：
&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;fs.default.name&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;hdfs://master_ip_here:9000&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;fs.</description>
    </item>
    
    <item>
      <title>Linux Shell 语法(上)</title>
      <link>https://blog.fifcom.cn/posts/linux-shell-manual-1/</link>
      <pubDate>Mon, 03 Jan 2022 00:00:00 +0800</pubDate>
      
      <guid>https://blog.fifcom.cn/posts/linux-shell-manual-1/</guid>
      <description>Hello World! #! /bin/bashecho &amp;#34;Hello World!&amp;#34; 第一行是注释，声明用bash作为此脚本的解释器。注释分为两种:
# 单行注释:&amp;lt;&amp;lt;EOF多行注释EOF# 其中EOF可以替换为任意字符串 第二行的echo用于在控制台上输出内容并自动换行。准确的说，echo会输出传递给它的所有参数，以空格分隔。 &amp;quot;Hello World!&amp;quot; 是字符串，后面会详细介绍。
保存后可以使用bash filename.sh运行，或者:
chmod +x filename.sh # 赋予可执行权限./hw.sh # 作为可执行文件执行 变量  定义可变变量  v1=123v2=123.4v3=fifcomv4=&amp;#34;fifcom&amp;#34;v5=&amp;#39;fifcom&amp;#39;declare v6=fifcom# 变量默认为字符串，在使用时会根据需求自动转换类型# 注意：定义变量时等号的左右两边不可以有空格# 否则可能被解析器作为系统命令来执行 定义只读变量  readonly v1=&amp;#34;immut&amp;#34; # 定义只读变量v2=&amp;#34;mut&amp;#34;readonly v2 # 转换为只读变量declare -r v3=&amp;#34;immut&amp;#34; # 定义只读变量 使用变量  使用变量需要加上$或${}符号。 ${} 可以帮助解释器识别变量名
v1=fifcomecho $v1 # 输出fifcomecho ${v1} # 输出fifcomecho ${v1}v1 # 输出fifcomv1 删除变量  用unset删除变量。只读变量不可删除。</description>
    </item>
    
    <item>
      <title>CF/YJS CDN下使用PHP获取客户端真实IP地址</title>
      <link>https://blog.fifcom.cn/posts/cloud-cf-yjs-cdn-get-remote-addr/</link>
      <pubDate>Sat, 27 Nov 2021 00:00:00 +0800</pubDate>
      
      <guid>https://blog.fifcom.cn/posts/cloud-cf-yjs-cdn-get-remote-addr/</guid>
      <description>打印一下$_SERVER
&amp;lt;?phpecho json_encode($_SERVER, JSON_UNESCAPED_UNICODE);?&amp;gt; 有这几项值得注意
&amp;#34;HTTP_X_FORWARDED_FOR&amp;#34;:&amp;#34;[YOUR_IP]&amp;#34;,&amp;#34;HTTP_CF_IPCOUNTRY&amp;#34;:&amp;#34;[YOUR_COUNTRY]&amp;#34;,&amp;#34;HTTP_CF_CONNECTING_IP&amp;#34;:&amp;#34;[YOUR_IP]&amp;#34;,&amp;#34;HTTP_CDN_LOOP&amp;#34;:&amp;#34;cloudflare&amp;#34;, 即 `$_SERVER[&amp;quot;HTTP_X_FORWARDED_FOR&amp;quot;]` 就是客户端的真实IP地址
另外不支持IPv6的服务器也可以通过 $_SERVER[&amp;quot;HTTP_X_FORWARDED_FOR&amp;quot;] 来获取客户端的IPv6地址
自动根据情况获取IP:
$ip = isset($_SERVER[&amp;#39;HTTP_X_FORWARDED_FOR&amp;#39;]) &amp;amp;&amp;amp; $_SERVER[&amp;#39;HTTP_X_FORWARDED_FOR&amp;#39;] != &amp;#34;&amp;#34; ? $_SERVER[&amp;#39;HTTP_X_FORWARDED_FOR&amp;#39;] : $_SERVER[&amp;#39;REMOTE_ADDR&amp;#39;]; </description>
    </item>
    
    <item>
      <title>nginx 反代解决CORS问题</title>
      <link>https://blog.fifcom.cn/posts/cloud-nginx-reverse-proxy-allow-cors/</link>
      <pubDate>Sat, 27 Nov 2021 00:00:00 +0800</pubDate>
      
      <guid>https://blog.fifcom.cn/posts/cloud-nginx-reverse-proxy-allow-cors/</guid>
      <description>同源策略规定，浏览器的ajax只能访问跟它的HTML页面同源（相同域名或IP）的资源。
使用ajax时容易遇到跨域问题
就像这样
最好的方法自然是修改服务端代码，如php里的
header(&amp;#34;Access-Control-Allow-Origin: blog.fifcom.cn&amp;#34;); // 允许指定域名跨域header(&amp;#34;Access-Control-Allow-Origin: *&amp;#34;); // 允许任意域名跨域 不过有时候无法修改服务端代码，这时候就需要用 nginx 反代来解决了。编辑 nginx.conf 并添加：
server {listen 443 ssl;# listen 80; # 如为http则监听80ssl on;ssl_certificate /path/pub.crt; #ssl证书ssl_certificate_key /path/pri.key;#ssl私钥ssl_protocols TLSv1 TLSv1.1 TLSv1.2;ssl_ciphers HIGH:!aNULL:!MD5;server_name example-proxy.fifcom.cn; #你的反代域名location / {proxy_pass https://example.fifcom.cn; # 被代理的域名(源站)proxy_http_version 1.1;# proxy_set_header Referer https://example.fifcom.cn; # 源站检测referer时需加上# 一些其他设置proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;proxy_set_header REMOTE-HOST $remote_addr;proxy_buffering off; proxy_cache off;proxy_set_header X-Forwarded-Proto $scheme;add_header X-Cache $upstream_cache_status;add_header Access-Control-Allow-Methods *; # 允许所有跨域方法add_header Access-Control-Max-Age 3600; # 缓存,否则会发送两次请求# add_header Access-Control-Allow-Credentials true; # 请求包含cookie时要开启add_header Access-Control-Allow-Origin $http_origin;# 允许客户端跨域,不用*是因为请求包含cookie时不支持允许所有add_header Access-Control-Allow-Headers $http_access_control_request_headers; # 自动配置允许的headerif ($request_method = OPTIONS){return 200;# 判断请求类型是否为预检命令}proxy_set_header Host example.</description>
    </item>
    
    <item>
      <title>在Debian10上安装AdoptOpenJDK16</title>
      <link>https://blog.fifcom.cn/posts/linux-install-jdk16-at-debian10/</link>
      <pubDate>Tue, 06 Jul 2021 00:00:00 +0800</pubDate>
      
      <guid>https://blog.fifcom.cn/posts/linux-install-jdk16-at-debian10/</guid>
      <description>首先卸载掉自带的openjdk
apt-get remove openjdk*apt-get purge openjdk* 再配置一下源
 信任GnuPG公钥  wget -qO - https://adoptopenjdk.jfrog.io/adoptopenjdk/api/gpg/key/public | sudo apt-key add - 在/etc/apt/sources.list.d/AdoptOpenJDK.list里添加以下内容：  deb http://mirrors.tuna.tsinghua.edu.cn/AdoptOpenJDK/deb buster main (编辑器可以用vim : vi /etc/apt/sources.list.d/AdoptOpenJDK.list ,添加完成后输入 :wq! 退出)
更新apt  sudo apt-get update 下载并安装 AdoptOpenJDK 16  sudo apt install adoptopenjdk-16-openj9 完成！
 参考 AdoptOpenJDK 镜像使用帮助
 </description>
    </item>
    
    <item>
      <title>友情链接</title>
      <link>https://blog.fifcom.cn/posts/friends-link/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0800</pubDate>
      
      <guid>https://blog.fifcom.cn/posts/friends-link/</guid>
      <description>淅筱TUEP的博客
粉身若叶のBlog</description>
    </item>
    
  </channel>
</rss>
